{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DenseNet - cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qndmzL532WiE",
        "colab_type": "text"
      },
      "source": [
        "#CNN on CIFR Assignment:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWRTY8iu2ZLV",
        "colab_type": "text"
      },
      "source": [
        "1.  Please visit this link to access the state-of-art DenseNet code for reference - DenseNet - cifar10 notebook link\n",
        "2.  You need to create a copy of this and \"retrain\" this model to achieve 90+ test accuracy. \n",
        "3.  You cannot use Dense Layers (also called fully connected layers), or DropOut.\n",
        "4.  You MUST use Image Augmentation Techniques.\n",
        "5.  You cannot use an already trained model as a beginning points, you have to initilize as your own\n",
        "6.  You cannot run the program for more than 300 Epochs, and it should be clear from your log, that you have only used 300 Epochs\n",
        "7.  You cannot use test images for training the model.\n",
        "8.  You cannot change the general architecture of DenseNet (which means you must use Dense Block, Transition and Output blocks as mentioned in the code)\n",
        "9.  You are free to change Convolution types (e.g. from 3x3 normal convolution to Depthwise Separable, etc)\n",
        "10. You cannot have more than 1 Million parameters in total\n",
        "11. You are free to move the code from Keras to Tensorflow, Pytorch, MXNET etc. \n",
        "12. You can use any optimization algorithm you need. \n",
        "13. You can checkpoint your model and retrain the model from that checkpoint so that no need of training the model from first if you lost at any epoch while training. You can directly load that model and Train from that epoch. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wVIx_KIigxPV",
        "colab": {}
      },
      "source": [
        "# import keras\n",
        "# from keras.datasets import cifar10\n",
        "# from keras.models import Model, Sequential\n",
        "# from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "# from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "# from keras.layers import Concatenate\n",
        "# from keras.optimizers import Adam\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Flatten\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UNHw6luQg3gc",
        "colab": {}
      },
      "source": [
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dsO_yGxcg5D8",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "l = 40\n",
        "num_filter = 12\n",
        "compression = 0.5\n",
        "dropout_rate = 0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mB7o3zu1g6eT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9ca24e95-2f9b-463d-c7ad-753b9ed1eb98"
      },
      "source": [
        "# Load CIFAR10 Data\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 4s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lAk_Mw_5-rn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da66b517-21b9-45f0-893c-7d9a5598ca83"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVkpgHsc5-rp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "88d1a75d-386c-49c0-a47f-d3b5555c75d1"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ee-sge5Kg7vr",
        "colab": {}
      },
      "source": [
        "# Dense Block\n",
        "def denseblock(input, num_filter = 12, dropout_rate = 0.1):\n",
        "    global compression\n",
        "    temp = input\n",
        "    for _ in range(l): \n",
        "        BatchNorm = layers.BatchNormalization()(temp)\n",
        "        relu = layers.Activation('relu')(BatchNorm)\n",
        "        Conv2D_3_3 = layers.Conv2D(int(num_filter*compression), (4,4), use_bias=False , kernel_initializer='glorot_uniform',padding='same')(relu)\n",
        "        if dropout_rate>0:\n",
        "            Conv2D_3_3 = layers.Dropout(dropout_rate)(Conv2D_3_3)\n",
        "        concat = layers.Concatenate(axis=-1)([temp,Conv2D_3_3])\n",
        "        \n",
        "        temp = concat\n",
        "        \n",
        "    return temp\n",
        "\n",
        "## transition Blosck\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.1):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    Conv2D_BottleNeck = layers.Conv2D(int(num_filter*compression), (1,1), use_bias=False, kernel_initializer='glorot_uniform' ,padding='same')(relu)\n",
        "    if dropout_rate>0:\n",
        "         Conv2D_BottleNeck = layers.Dropout(dropout_rate)(Conv2D_BottleNeck)\n",
        "    avg = layers.AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)\n",
        "    return avg\n",
        "\n",
        "#output layer\n",
        "def output_layer(input):\n",
        "    global compression\n",
        "    BatchNorm = layers.BatchNormalization()(input)\n",
        "    relu = layers.Activation('relu')(BatchNorm)\n",
        "    AvgPooling = layers.AveragePooling2D(pool_size=(2,2))(relu)\n",
        "    flat = layers.Flatten()(AvgPooling)\n",
        "    output = layers.Dense(num_classes, activation='softmax')(flat)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anPCpQWhhGb7",
        "colab": {}
      },
      "source": [
        "num_filter = 20\n",
        "dropout_rate = 0\n",
        "l = 12\n",
        "input = layers.Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = layers.Conv2D(64, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, 32, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, 25, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1kFh7pdxhNtT",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e2673a25-65f8-4efd-a344-2d76120a8f39"
      },
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 32, 32, 64)   1728        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 32, 32, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 32, 32, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 16)   16384       activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 32, 32, 80)   0           conv2d[0][0]                     \n",
            "                                                                 conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 80)   320         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 32, 32, 80)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 16)   20480       activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 32, 32, 96)   0           concatenate[0][0]                \n",
            "                                                                 conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 96)   384         concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 32, 32, 96)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 16)   24576       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 32, 32, 112)  0           concatenate_1[0][0]              \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 112)  448         concatenate_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 32, 32, 112)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 16)   28672       activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 32, 32, 128)  0           concatenate_2[0][0]              \n",
            "                                                                 conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 128)  512         concatenate_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 32, 32, 128)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 16)   32768       activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_4 (Concatenate)     (None, 32, 32, 144)  0           concatenate_3[0][0]              \n",
            "                                                                 conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 144)  576         concatenate_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 32, 32, 144)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 16)   36864       activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_5 (Concatenate)     (None, 32, 32, 160)  0           concatenate_4[0][0]              \n",
            "                                                                 conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 160)  640         concatenate_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 32, 32, 160)  0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 32, 32, 16)   40960       activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 32, 32, 176)  0           concatenate_5[0][0]              \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 32, 32, 176)  704         concatenate_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 32, 32, 176)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 32, 32, 16)   45056       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 32, 32, 192)  0           concatenate_6[0][0]              \n",
            "                                                                 conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 32, 32, 192)  768         concatenate_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 32, 32, 192)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 32, 32, 16)   49152       activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 32, 32, 208)  0           concatenate_7[0][0]              \n",
            "                                                                 conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 32, 32, 208)  832         concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 32, 32, 208)  0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 32, 32, 16)   53248       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 32, 32, 224)  0           concatenate_8[0][0]              \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 224)  896         concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 224)  0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 16)   57344       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_10 (Concatenate)    (None, 32, 32, 240)  0           concatenate_9[0][0]              \n",
            "                                                                 conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 240)  960         concatenate_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 240)  0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 16)   61440       activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_11 (Concatenate)    (None, 32, 32, 256)  0           concatenate_10[0][0]             \n",
            "                                                                 conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 256)  1024        concatenate_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 256)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 10)   2560        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 16, 16, 10)   0           conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 10)   40          average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 16, 16, 10)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 12)   1920        activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_12 (Concatenate)    (None, 16, 16, 22)   0           average_pooling2d[0][0]          \n",
            "                                                                 conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 22)   88          concatenate_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 16, 16, 22)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 12)   4224        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_13 (Concatenate)    (None, 16, 16, 34)   0           concatenate_12[0][0]             \n",
            "                                                                 conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 34)   136         concatenate_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 16, 16, 34)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 16, 16, 12)   6528        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_14 (Concatenate)    (None, 16, 16, 46)   0           concatenate_13[0][0]             \n",
            "                                                                 conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 16, 16, 46)   184         concatenate_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 16, 16, 46)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 16, 16, 12)   8832        activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_15 (Concatenate)    (None, 16, 16, 58)   0           concatenate_14[0][0]             \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 16, 16, 58)   232         concatenate_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 16, 16, 58)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 16, 16, 12)   11136       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (None, 16, 16, 70)   0           concatenate_15[0][0]             \n",
            "                                                                 conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 16, 16, 70)   280         concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 16, 16, 70)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 16, 16, 12)   13440       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (None, 16, 16, 82)   0           concatenate_16[0][0]             \n",
            "                                                                 conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 82)   328         concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 82)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 16, 16, 12)   15744       activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (None, 16, 16, 94)   0           concatenate_17[0][0]             \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 94)   376         concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 94)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 12)   18048       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (None, 16, 16, 106)  0           concatenate_18[0][0]             \n",
            "                                                                 conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 106)  424         concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 106)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 12)   20352       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (None, 16, 16, 118)  0           concatenate_19[0][0]             \n",
            "                                                                 conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 118)  472         concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 118)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 12)   22656       activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (None, 16, 16, 130)  0           concatenate_20[0][0]             \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 130)  520         concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 130)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 12)   24960       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (None, 16, 16, 142)  0           concatenate_21[0][0]             \n",
            "                                                                 conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 142)  568         concatenate_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 142)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 12)   27264       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_23 (Concatenate)    (None, 16, 16, 154)  0           concatenate_22[0][0]             \n",
            "                                                                 conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 154)  616         concatenate_23[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 154)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 10)   1540        activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 8, 8, 10)     0           conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 10)     40          average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 8, 8, 10)     0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 10)     1600        activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_24 (Concatenate)    (None, 8, 8, 20)     0           average_pooling2d_1[0][0]        \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 20)     80          concatenate_24[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 8, 8, 20)     0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 10)     3200        activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_25 (Concatenate)    (None, 8, 8, 30)     0           concatenate_24[0][0]             \n",
            "                                                                 conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 30)     120         concatenate_25[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 8, 8, 30)     0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 8, 8, 10)     4800        activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_26 (Concatenate)    (None, 8, 8, 40)     0           concatenate_25[0][0]             \n",
            "                                                                 conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 8, 8, 40)     160         concatenate_26[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 8, 8, 40)     0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 8, 8, 10)     6400        activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_27 (Concatenate)    (None, 8, 8, 50)     0           concatenate_26[0][0]             \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 8, 8, 50)     200         concatenate_27[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 8, 8, 50)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 8, 8, 10)     8000        activation_30[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_28 (Concatenate)    (None, 8, 8, 60)     0           concatenate_27[0][0]             \n",
            "                                                                 conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 8, 8, 60)     240         concatenate_28[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 8, 8, 60)     0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 8, 8, 10)     9600        activation_31[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_29 (Concatenate)    (None, 8, 8, 70)     0           concatenate_28[0][0]             \n",
            "                                                                 conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 8, 8, 70)     280         concatenate_29[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 8, 8, 70)     0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 8, 8, 10)     11200       activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_30 (Concatenate)    (None, 8, 8, 80)     0           concatenate_29[0][0]             \n",
            "                                                                 conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 8, 8, 80)     320         concatenate_30[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 8, 8, 80)     0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 8, 8, 10)     12800       activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_31 (Concatenate)    (None, 8, 8, 90)     0           concatenate_30[0][0]             \n",
            "                                                                 conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 8, 8, 90)     360         concatenate_31[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 8, 8, 90)     0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 8, 8, 10)     14400       activation_34[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_32 (Concatenate)    (None, 8, 8, 100)    0           concatenate_31[0][0]             \n",
            "                                                                 conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 8, 8, 100)    400         concatenate_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 8, 8, 100)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 8, 8, 10)     16000       activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_33 (Concatenate)    (None, 8, 8, 110)    0           concatenate_32[0][0]             \n",
            "                                                                 conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 8, 8, 110)    440         concatenate_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 8, 8, 110)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 8, 8, 10)     17600       activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_34 (Concatenate)    (None, 8, 8, 120)    0           concatenate_33[0][0]             \n",
            "                                                                 conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 8, 8, 120)    480         concatenate_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 8, 8, 120)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 8, 8, 10)     19200       activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_35 (Concatenate)    (None, 8, 8, 130)    0           concatenate_34[0][0]             \n",
            "                                                                 conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 8, 8, 130)    520         concatenate_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 8, 8, 130)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 8, 8, 10)     1300        activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 4, 4, 10)     0           conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 4, 4, 10)     40          average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 4, 4, 10)     0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 4, 4, 10)     1600        activation_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_36 (Concatenate)    (None, 4, 4, 20)     0           average_pooling2d_2[0][0]        \n",
            "                                                                 conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 4, 4, 20)     80          concatenate_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 4, 4, 20)     0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 4, 4, 10)     3200        activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_37 (Concatenate)    (None, 4, 4, 30)     0           concatenate_36[0][0]             \n",
            "                                                                 conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 4, 4, 30)     120         concatenate_37[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 4, 4, 30)     0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 4, 4, 10)     4800        activation_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_38 (Concatenate)    (None, 4, 4, 40)     0           concatenate_37[0][0]             \n",
            "                                                                 conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 4, 4, 40)     160         concatenate_38[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 4, 4, 40)     0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 4, 4, 10)     6400        activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_39 (Concatenate)    (None, 4, 4, 50)     0           concatenate_38[0][0]             \n",
            "                                                                 conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 4, 4, 50)     200         concatenate_39[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 4, 4, 50)     0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 4, 4, 10)     8000        activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (None, 4, 4, 60)     0           concatenate_39[0][0]             \n",
            "                                                                 conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 4, 4, 60)     240         concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 4, 4, 60)     0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 4, 4, 10)     9600        activation_44[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (None, 4, 4, 70)     0           concatenate_40[0][0]             \n",
            "                                                                 conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 4, 4, 70)     280         concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 4, 4, 70)     0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 4, 4, 10)     11200       activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (None, 4, 4, 80)     0           concatenate_41[0][0]             \n",
            "                                                                 conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 4, 4, 80)     320         concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 4, 4, 80)     0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 4, 4, 10)     12800       activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (None, 4, 4, 90)     0           concatenate_42[0][0]             \n",
            "                                                                 conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 4, 4, 90)     360         concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 4, 4, 90)     0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 4, 4, 10)     14400       activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (None, 4, 4, 100)    0           concatenate_43[0][0]             \n",
            "                                                                 conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 4, 4, 100)    400         concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 4, 4, 100)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 4, 4, 10)     16000       activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (None, 4, 4, 110)    0           concatenate_44[0][0]             \n",
            "                                                                 conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 4, 4, 110)    440         concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 4, 4, 110)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 4, 4, 10)     17600       activation_49[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (None, 4, 4, 120)    0           concatenate_45[0][0]             \n",
            "                                                                 conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 4, 4, 120)    480         concatenate_46[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 4, 4, 120)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 4, 4, 10)     19200       activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (None, 4, 4, 130)    0           concatenate_46[0][0]             \n",
            "                                                                 conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 4, 4, 130)    520         concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 4, 4, 130)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 2, 2, 130)    0           activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 520)          0           average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 10)           5210        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 923,850\n",
            "Trainable params: 913,918\n",
            "Non-trainable params: 9,932\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Aqzk9AFXb1y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f7c36a3-8ae7-4a4e-b87e-608dc9b36149"
      },
      "source": [
        "print(len(model.layers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b4XOsW3ahSkL",
        "colab": {}
      },
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam() ,metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMyr1Wi1Pdvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adding \n",
        "import numpy as np\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "#z-score\n",
        "mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "std = np.std(X_train,axis=(0,1,2,3))\n",
        "X_train = (X_train-mean)/(std+1e-7)\n",
        "X_test = (X_test-mean)/(std+1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4zJZg8hiEEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    )\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k6WDkmq_ouM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "90dba4ce-fbb5-4f25-b229-64f40faa3b02"
      },
      "source": [
        "#adding \n",
        "import os\n",
        "import datetime\n",
        "#reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy',patience=3, factor = 0.8) \n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.5,patience=1)\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', mode='max')\n",
        "import warnings\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint # from Transfer Learning assignment\n",
        "\n",
        "checkpoint_path = \"/gdrive/My Drive/cifar/logs/my_model_weights.hdf5\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='/gdrive/My Drive/cifar/logs/my_model_weights{epoch}.hdf5',  verbose=1, save_weights_only=False,save_best_only=False, mode='auto') \n",
        "log_dir=\"/gdrive/My Drive/cifar/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True,write_grads=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`write_grads` will be ignored in TensorFlow 2.0 for the `TensorBoard` Callback.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiUjQ3BNRowd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p saved_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "crhGk7kEhXAz",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "model.fit(X_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=100,\n",
        "                    verbose=1, \n",
        "                    validation_data=(X_test, y_test), callbacks= [reduce_lr,tensorboard_callback])\n",
        "\n",
        "model.save('/gdrive/My Drive/cifar/my_model') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZcWydmIVhZGr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ff396c98-1c17-48e6-dcda-3b3b46a1a933"
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 2s 7ms/step - loss: 1.0695 - accuracy: 0.6558\n",
            "Test loss: 1.0694855451583862\n",
            "Test accuracy: 0.6557999849319458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-S4tIzOFN90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch, lr):\n",
        "  lr = 0.0005\n",
        "  if epoch > 25  :\n",
        "    lr = 0.0006\n",
        "  return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYwT7WOualMV",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9C2MSPcYSghQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de794244-3d65-4fee-cb46-0eab782f0b6f"
      },
      "source": [
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "model.fit_generator(datagen.flow(X_train,y_train,batch_size=64), steps_per_epoch=300,\n",
        "                    epochs=100, verbose=1, validation_data=(X_test, y_test), callbacks= [reduce_lr,tensorboard_callback, callback])\n",
        "model.save('/gdrive/My Drive/cifar/my_model.h5') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "  2/300 [..............................] - ETA: 1:13 - loss: 0.9923 - accuracy: 0.6719WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0883s vs `on_train_batch_end` time: 0.3996s). Check your callbacks.\n",
            "300/300 [==============================] - 48s 159ms/step - loss: 1.3601 - accuracy: 0.5082 - val_loss: 1.3876 - val_accuracy: 0.5106\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 1.2318 - accuracy: 0.5534 - val_loss: 1.3018 - val_accuracy: 0.5366\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 1.1433 - accuracy: 0.5908 - val_loss: 1.1205 - val_accuracy: 0.6068\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 1.0528 - accuracy: 0.6212 - val_loss: 1.4086 - val_accuracy: 0.5560\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 1.0133 - accuracy: 0.6431 - val_loss: 1.3797 - val_accuracy: 0.5768\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.9710 - accuracy: 0.6527 - val_loss: 1.2345 - val_accuracy: 0.5951\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.9309 - accuracy: 0.6683 - val_loss: 1.3296 - val_accuracy: 0.5619\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.8809 - accuracy: 0.6874 - val_loss: 1.0248 - val_accuracy: 0.6522\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.8503 - accuracy: 0.7016 - val_loss: 0.9746 - val_accuracy: 0.6586\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.8184 - accuracy: 0.7079 - val_loss: 0.8495 - val_accuracy: 0.7026\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.7959 - accuracy: 0.7205 - val_loss: 1.0870 - val_accuracy: 0.6459\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.7775 - accuracy: 0.7273 - val_loss: 1.0093 - val_accuracy: 0.6674\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.7464 - accuracy: 0.7396 - val_loss: 0.8550 - val_accuracy: 0.7095\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.7273 - accuracy: 0.7476 - val_loss: 0.8325 - val_accuracy: 0.7151\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.7041 - accuracy: 0.7512 - val_loss: 0.7694 - val_accuracy: 0.7388\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.6824 - accuracy: 0.7618 - val_loss: 1.0244 - val_accuracy: 0.6678\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.6852 - accuracy: 0.7621 - val_loss: 0.9699 - val_accuracy: 0.6721\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.6559 - accuracy: 0.7710 - val_loss: 0.9409 - val_accuracy: 0.7005\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.6469 - accuracy: 0.7726 - val_loss: 0.8452 - val_accuracy: 0.7135\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.6252 - accuracy: 0.7821 - val_loss: 0.6910 - val_accuracy: 0.7678\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.6225 - accuracy: 0.7837 - val_loss: 0.7543 - val_accuracy: 0.7490\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.6117 - accuracy: 0.7869 - val_loss: 0.7581 - val_accuracy: 0.7456\n",
            "Epoch 23/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.5893 - accuracy: 0.7952 - val_loss: 0.7270 - val_accuracy: 0.7528\n",
            "Epoch 24/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.5893 - accuracy: 0.7959 - val_loss: 0.8077 - val_accuracy: 0.7311\n",
            "Epoch 25/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.5755 - accuracy: 0.7992 - val_loss: 0.7379 - val_accuracy: 0.7557\n",
            "Epoch 26/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.5731 - accuracy: 0.8033 - val_loss: 0.6601 - val_accuracy: 0.7804\n",
            "Epoch 27/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.5026 - accuracy: 0.8282 - val_loss: 0.5395 - val_accuracy: 0.8149\n",
            "Epoch 28/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.4808 - accuracy: 0.8369 - val_loss: 0.5557 - val_accuracy: 0.8075\n",
            "Epoch 29/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4717 - accuracy: 0.8362 - val_loss: 0.4955 - val_accuracy: 0.8301\n",
            "Epoch 30/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4685 - accuracy: 0.8383 - val_loss: 0.5295 - val_accuracy: 0.8155\n",
            "Epoch 31/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4605 - accuracy: 0.8387 - val_loss: 0.5259 - val_accuracy: 0.8212\n",
            "Epoch 32/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.4421 - accuracy: 0.8457 - val_loss: 0.5231 - val_accuracy: 0.8230\n",
            "Epoch 33/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.4490 - accuracy: 0.8456 - val_loss: 0.5292 - val_accuracy: 0.8212\n",
            "Epoch 34/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.4408 - accuracy: 0.8501 - val_loss: 0.5930 - val_accuracy: 0.8053\n",
            "Epoch 35/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.4420 - accuracy: 0.8458 - val_loss: 0.4982 - val_accuracy: 0.8321\n",
            "Epoch 36/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4322 - accuracy: 0.8526 - val_loss: 0.5150 - val_accuracy: 0.8271\n",
            "Epoch 37/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4338 - accuracy: 0.8498 - val_loss: 0.5222 - val_accuracy: 0.8254\n",
            "Epoch 38/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4271 - accuracy: 0.8497 - val_loss: 0.4683 - val_accuracy: 0.8433\n",
            "Epoch 39/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4195 - accuracy: 0.8554 - val_loss: 0.4932 - val_accuracy: 0.8320\n",
            "Epoch 40/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4244 - accuracy: 0.8516 - val_loss: 0.4916 - val_accuracy: 0.8372\n",
            "Epoch 41/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.4128 - accuracy: 0.8580 - val_loss: 0.4985 - val_accuracy: 0.8342\n",
            "Epoch 42/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4242 - accuracy: 0.8545 - val_loss: 0.4663 - val_accuracy: 0.8409\n",
            "Epoch 43/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4258 - accuracy: 0.8528 - val_loss: 0.5081 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4087 - accuracy: 0.8571 - val_loss: 0.5526 - val_accuracy: 0.8202\n",
            "Epoch 45/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4079 - accuracy: 0.8577 - val_loss: 0.5051 - val_accuracy: 0.8329\n",
            "Epoch 46/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4109 - accuracy: 0.8597 - val_loss: 0.4726 - val_accuracy: 0.8425\n",
            "Epoch 47/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.4104 - accuracy: 0.8582 - val_loss: 0.5461 - val_accuracy: 0.8222\n",
            "Epoch 48/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3907 - accuracy: 0.8672 - val_loss: 0.5027 - val_accuracy: 0.8343\n",
            "Epoch 49/100\n",
            "300/300 [==============================] - 47s 156ms/step - loss: 0.4113 - accuracy: 0.8563 - val_loss: 0.4649 - val_accuracy: 0.8448\n",
            "Epoch 50/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3949 - accuracy: 0.8640 - val_loss: 0.5009 - val_accuracy: 0.8337\n",
            "Epoch 51/100\n",
            "300/300 [==============================] - 47s 156ms/step - loss: 0.3943 - accuracy: 0.8619 - val_loss: 0.5065 - val_accuracy: 0.8324\n",
            "Epoch 52/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3895 - accuracy: 0.8640 - val_loss: 0.5219 - val_accuracy: 0.8277\n",
            "Epoch 53/100\n",
            "300/300 [==============================] - 47s 156ms/step - loss: 0.3974 - accuracy: 0.8603 - val_loss: 0.4889 - val_accuracy: 0.8373\n",
            "Epoch 54/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3928 - accuracy: 0.8622 - val_loss: 0.5251 - val_accuracy: 0.8259\n",
            "Epoch 55/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3862 - accuracy: 0.8637 - val_loss: 0.4510 - val_accuracy: 0.8496\n",
            "Epoch 56/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3931 - accuracy: 0.8631 - val_loss: 0.4416 - val_accuracy: 0.8511\n",
            "Epoch 57/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3861 - accuracy: 0.8628 - val_loss: 0.4818 - val_accuracy: 0.8391\n",
            "Epoch 58/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3822 - accuracy: 0.8676 - val_loss: 0.4858 - val_accuracy: 0.8379\n",
            "Epoch 59/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3763 - accuracy: 0.8656 - val_loss: 0.4752 - val_accuracy: 0.8433\n",
            "Epoch 60/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3795 - accuracy: 0.8682 - val_loss: 0.5027 - val_accuracy: 0.8362\n",
            "Epoch 61/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3749 - accuracy: 0.8709 - val_loss: 0.5400 - val_accuracy: 0.8234\n",
            "Epoch 62/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3764 - accuracy: 0.8687 - val_loss: 0.4763 - val_accuracy: 0.8462\n",
            "Epoch 63/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3770 - accuracy: 0.8708 - val_loss: 0.4668 - val_accuracy: 0.8471\n",
            "Epoch 64/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3644 - accuracy: 0.8732 - val_loss: 0.4619 - val_accuracy: 0.8476\n",
            "Epoch 65/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3727 - accuracy: 0.8695 - val_loss: 0.4929 - val_accuracy: 0.8387\n",
            "Epoch 66/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3625 - accuracy: 0.8720 - val_loss: 0.4461 - val_accuracy: 0.8542\n",
            "Epoch 67/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3706 - accuracy: 0.8721 - val_loss: 0.4440 - val_accuracy: 0.8538\n",
            "Epoch 68/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3588 - accuracy: 0.8780 - val_loss: 0.4850 - val_accuracy: 0.8403\n",
            "Epoch 69/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3626 - accuracy: 0.8748 - val_loss: 0.4830 - val_accuracy: 0.8424\n",
            "Epoch 70/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3605 - accuracy: 0.8742 - val_loss: 0.4741 - val_accuracy: 0.8453\n",
            "Epoch 71/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3612 - accuracy: 0.8759 - val_loss: 0.4551 - val_accuracy: 0.8527\n",
            "Epoch 72/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3610 - accuracy: 0.8735 - val_loss: 0.4487 - val_accuracy: 0.8530\n",
            "Epoch 73/100\n",
            "300/300 [==============================] - 48s 159ms/step - loss: 0.3614 - accuracy: 0.8722 - val_loss: 0.4922 - val_accuracy: 0.8416\n",
            "Epoch 74/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3580 - accuracy: 0.8758 - val_loss: 0.5011 - val_accuracy: 0.8380\n",
            "Epoch 75/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3531 - accuracy: 0.8759 - val_loss: 0.4551 - val_accuracy: 0.8492\n",
            "Epoch 76/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3510 - accuracy: 0.8769 - val_loss: 0.4259 - val_accuracy: 0.8588\n",
            "Epoch 77/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3569 - accuracy: 0.8770 - val_loss: 0.4944 - val_accuracy: 0.8426\n",
            "Epoch 78/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3370 - accuracy: 0.8851 - val_loss: 0.4628 - val_accuracy: 0.8486\n",
            "Epoch 79/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3550 - accuracy: 0.8789 - val_loss: 0.4625 - val_accuracy: 0.8501\n",
            "Epoch 80/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3530 - accuracy: 0.8772 - val_loss: 0.4373 - val_accuracy: 0.8568\n",
            "Epoch 81/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3512 - accuracy: 0.8776 - val_loss: 0.5323 - val_accuracy: 0.8272\n",
            "Epoch 82/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3437 - accuracy: 0.8799 - val_loss: 0.4693 - val_accuracy: 0.8477\n",
            "Epoch 83/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3384 - accuracy: 0.8813 - val_loss: 0.4691 - val_accuracy: 0.8475\n",
            "Epoch 84/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3383 - accuracy: 0.8834 - val_loss: 0.4460 - val_accuracy: 0.8547\n",
            "Epoch 85/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3419 - accuracy: 0.8826 - val_loss: 0.4446 - val_accuracy: 0.8534\n",
            "Epoch 86/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3378 - accuracy: 0.8813 - val_loss: 0.4348 - val_accuracy: 0.8577\n",
            "Epoch 87/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3348 - accuracy: 0.8849 - val_loss: 0.4531 - val_accuracy: 0.8488\n",
            "Epoch 88/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3319 - accuracy: 0.8846 - val_loss: 0.4603 - val_accuracy: 0.8514\n",
            "Epoch 89/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3306 - accuracy: 0.8829 - val_loss: 0.4727 - val_accuracy: 0.8458\n",
            "Epoch 90/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3294 - accuracy: 0.8849 - val_loss: 0.4228 - val_accuracy: 0.8624\n",
            "Epoch 91/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3231 - accuracy: 0.8888 - val_loss: 0.4523 - val_accuracy: 0.8529\n",
            "Epoch 92/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3217 - accuracy: 0.8884 - val_loss: 0.4626 - val_accuracy: 0.8510\n",
            "Epoch 93/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3300 - accuracy: 0.8851 - val_loss: 0.4723 - val_accuracy: 0.8498\n",
            "Epoch 94/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3260 - accuracy: 0.8854 - val_loss: 0.5075 - val_accuracy: 0.8352\n",
            "Epoch 95/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3236 - accuracy: 0.8897 - val_loss: 0.4664 - val_accuracy: 0.8457\n",
            "Epoch 96/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3197 - accuracy: 0.8896 - val_loss: 0.5439 - val_accuracy: 0.8291\n",
            "Epoch 97/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3108 - accuracy: 0.8896 - val_loss: 0.4671 - val_accuracy: 0.8499\n",
            "Epoch 98/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3174 - accuracy: 0.8894 - val_loss: 0.4749 - val_accuracy: 0.8483\n",
            "Epoch 99/100\n",
            "300/300 [==============================] - 47s 158ms/step - loss: 0.3185 - accuracy: 0.8878 - val_loss: 0.4702 - val_accuracy: 0.8490\n",
            "Epoch 100/100\n",
            "300/300 [==============================] - 47s 157ms/step - loss: 0.3217 - accuracy: 0.8883 - val_loss: 0.4671 - val_accuracy: 0.8496\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG22NjhRWUL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cec8daa7-d8a9-4ace-ad16-028792e99771"
      },
      "source": [
        "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "model.fit_generator(datagen.flow(X_train,y_train,batch_size=128), steps_per_epoch=300,\n",
        "                    epochs=100, verbose=1, validation_data=(X_test, y_test), callbacks= [reduce_lr,tensorboard_callback, callback])\n",
        "model.save('/gdrive/My Drive/cifar/my_model.h5') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "  2/300 [..............................] - ETA: 1:43 - loss: 0.4180 - accuracy: 0.8398WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1351s vs `on_train_batch_end` time: 0.5563s). Check your callbacks.\n",
            "300/300 [==============================] - 77s 256ms/step - loss: 0.5547 - accuracy: 0.8054 - val_loss: 0.7250 - val_accuracy: 0.7793\n",
            "Epoch 2/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.5310 - accuracy: 0.8160 - val_loss: 1.1765 - val_accuracy: 0.6913\n",
            "Epoch 3/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.5086 - accuracy: 0.8227 - val_loss: 0.7394 - val_accuracy: 0.7582\n",
            "Epoch 4/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.4863 - accuracy: 0.8297 - val_loss: 0.6180 - val_accuracy: 0.7951\n",
            "Epoch 5/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.4761 - accuracy: 0.8352 - val_loss: 0.7651 - val_accuracy: 0.7632\n",
            "Epoch 6/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.4634 - accuracy: 0.8372 - val_loss: 0.7212 - val_accuracy: 0.7785\n",
            "Epoch 7/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.4392 - accuracy: 0.8484 - val_loss: 0.5424 - val_accuracy: 0.8219\n",
            "Epoch 8/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.4381 - accuracy: 0.8464 - val_loss: 0.6499 - val_accuracy: 0.7950\n",
            "Epoch 9/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.4217 - accuracy: 0.8530 - val_loss: 1.0235 - val_accuracy: 0.7320\n",
            "Epoch 10/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.4135 - accuracy: 0.8581 - val_loss: 0.7117 - val_accuracy: 0.7732\n",
            "Epoch 11/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.4065 - accuracy: 0.8586 - val_loss: 0.5887 - val_accuracy: 0.8146\n",
            "Epoch 12/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3942 - accuracy: 0.8635 - val_loss: 0.5000 - val_accuracy: 0.8299\n",
            "Epoch 13/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3860 - accuracy: 0.8680 - val_loss: 0.5534 - val_accuracy: 0.8153\n",
            "Epoch 14/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3870 - accuracy: 0.8674 - val_loss: 0.5457 - val_accuracy: 0.8196\n",
            "Epoch 15/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3671 - accuracy: 0.8724 - val_loss: 0.5309 - val_accuracy: 0.8279\n",
            "Epoch 16/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3620 - accuracy: 0.8751 - val_loss: 0.5650 - val_accuracy: 0.8264\n",
            "Epoch 17/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3592 - accuracy: 0.8747 - val_loss: 0.6335 - val_accuracy: 0.8088\n",
            "Epoch 18/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3492 - accuracy: 0.8801 - val_loss: 0.4458 - val_accuracy: 0.8551\n",
            "Epoch 19/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.3434 - accuracy: 0.8803 - val_loss: 0.5040 - val_accuracy: 0.8341\n",
            "Epoch 20/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3368 - accuracy: 0.8837 - val_loss: 0.5538 - val_accuracy: 0.8362\n",
            "Epoch 21/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3300 - accuracy: 0.8865 - val_loss: 0.4358 - val_accuracy: 0.8551\n",
            "Epoch 22/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.3273 - accuracy: 0.8863 - val_loss: 0.5702 - val_accuracy: 0.8269\n",
            "Epoch 23/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.3189 - accuracy: 0.8881 - val_loss: 0.5501 - val_accuracy: 0.8238\n",
            "Epoch 24/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.3148 - accuracy: 0.8885 - val_loss: 0.6926 - val_accuracy: 0.8000\n",
            "Epoch 25/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.3033 - accuracy: 0.8941 - val_loss: 0.4739 - val_accuracy: 0.8483\n",
            "Epoch 26/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.2986 - accuracy: 0.8966 - val_loss: 0.8626 - val_accuracy: 0.7639\n",
            "Epoch 27/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.2423 - accuracy: 0.9151 - val_loss: 0.3505 - val_accuracy: 0.8824\n",
            "Epoch 28/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.2134 - accuracy: 0.9264 - val_loss: 0.3561 - val_accuracy: 0.8843\n",
            "Epoch 29/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.2115 - accuracy: 0.9253 - val_loss: 0.3259 - val_accuracy: 0.8926\n",
            "Epoch 30/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.2027 - accuracy: 0.9294 - val_loss: 0.3472 - val_accuracy: 0.8866\n",
            "Epoch 31/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1952 - accuracy: 0.9324 - val_loss: 0.3759 - val_accuracy: 0.8810\n",
            "Epoch 32/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1977 - accuracy: 0.9312 - val_loss: 0.3443 - val_accuracy: 0.8902\n",
            "Epoch 33/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1906 - accuracy: 0.9326 - val_loss: 0.3345 - val_accuracy: 0.8924\n",
            "Epoch 34/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1872 - accuracy: 0.9345 - val_loss: 0.3491 - val_accuracy: 0.8891\n",
            "Epoch 35/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1835 - accuracy: 0.9360 - val_loss: 0.3510 - val_accuracy: 0.8898\n",
            "Epoch 36/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1803 - accuracy: 0.9373 - val_loss: 0.3433 - val_accuracy: 0.8916\n",
            "Epoch 37/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1795 - accuracy: 0.9373 - val_loss: 0.3796 - val_accuracy: 0.8836\n",
            "Epoch 38/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1738 - accuracy: 0.9388 - val_loss: 0.3564 - val_accuracy: 0.8882\n",
            "Epoch 39/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1738 - accuracy: 0.9391 - val_loss: 0.3428 - val_accuracy: 0.8918\n",
            "Epoch 40/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1726 - accuracy: 0.9409 - val_loss: 0.3684 - val_accuracy: 0.8866\n",
            "Epoch 41/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1707 - accuracy: 0.9413 - val_loss: 0.3452 - val_accuracy: 0.8905\n",
            "Epoch 42/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1711 - accuracy: 0.9396 - val_loss: 0.3499 - val_accuracy: 0.8891\n",
            "Epoch 43/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1649 - accuracy: 0.9428 - val_loss: 0.3420 - val_accuracy: 0.8915\n",
            "Epoch 44/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1675 - accuracy: 0.9407 - val_loss: 0.3483 - val_accuracy: 0.8912\n",
            "Epoch 45/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1661 - accuracy: 0.9415 - val_loss: 0.3468 - val_accuracy: 0.8892\n",
            "Epoch 46/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1637 - accuracy: 0.9420 - val_loss: 0.3363 - val_accuracy: 0.8954\n",
            "Epoch 47/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1623 - accuracy: 0.9434 - val_loss: 0.3297 - val_accuracy: 0.8958\n",
            "Epoch 48/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1576 - accuracy: 0.9459 - val_loss: 0.3324 - val_accuracy: 0.8936\n",
            "Epoch 49/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1568 - accuracy: 0.9453 - val_loss: 0.3547 - val_accuracy: 0.8904\n",
            "Epoch 50/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1578 - accuracy: 0.9443 - val_loss: 0.3426 - val_accuracy: 0.8925\n",
            "Epoch 51/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1515 - accuracy: 0.9462 - val_loss: 0.3376 - val_accuracy: 0.8941\n",
            "Epoch 52/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1553 - accuracy: 0.9465 - val_loss: 0.3583 - val_accuracy: 0.8903\n",
            "Epoch 53/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1518 - accuracy: 0.9470 - val_loss: 0.3428 - val_accuracy: 0.8930\n",
            "Epoch 54/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1525 - accuracy: 0.9465 - val_loss: 0.3581 - val_accuracy: 0.8935\n",
            "Epoch 55/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1496 - accuracy: 0.9469 - val_loss: 0.3548 - val_accuracy: 0.8925\n",
            "Epoch 56/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1515 - accuracy: 0.9466 - val_loss: 0.3352 - val_accuracy: 0.8950\n",
            "Epoch 57/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1464 - accuracy: 0.9489 - val_loss: 0.3543 - val_accuracy: 0.8915\n",
            "Epoch 58/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1428 - accuracy: 0.9495 - val_loss: 0.3845 - val_accuracy: 0.8863\n",
            "Epoch 59/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1424 - accuracy: 0.9500 - val_loss: 0.3512 - val_accuracy: 0.8924\n",
            "Epoch 60/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1425 - accuracy: 0.9494 - val_loss: 0.3488 - val_accuracy: 0.8943\n",
            "Epoch 61/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1425 - accuracy: 0.9507 - val_loss: 0.3720 - val_accuracy: 0.8890\n",
            "Epoch 62/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1402 - accuracy: 0.9511 - val_loss: 0.3683 - val_accuracy: 0.8901\n",
            "Epoch 63/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1394 - accuracy: 0.9520 - val_loss: 0.3527 - val_accuracy: 0.8944\n",
            "Epoch 64/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1427 - accuracy: 0.9506 - val_loss: 0.3599 - val_accuracy: 0.8932\n",
            "Epoch 65/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1420 - accuracy: 0.9491 - val_loss: 0.3532 - val_accuracy: 0.8937\n",
            "Epoch 66/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1404 - accuracy: 0.9510 - val_loss: 0.3708 - val_accuracy: 0.8899\n",
            "Epoch 67/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1389 - accuracy: 0.9509 - val_loss: 0.3454 - val_accuracy: 0.8959\n",
            "Epoch 68/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1349 - accuracy: 0.9511 - val_loss: 0.3525 - val_accuracy: 0.8968\n",
            "Epoch 69/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1348 - accuracy: 0.9527 - val_loss: 0.3556 - val_accuracy: 0.8951\n",
            "Epoch 70/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1305 - accuracy: 0.9544 - val_loss: 0.3709 - val_accuracy: 0.8923\n",
            "Epoch 71/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1334 - accuracy: 0.9526 - val_loss: 0.3553 - val_accuracy: 0.8940\n",
            "Epoch 72/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1341 - accuracy: 0.9536 - val_loss: 0.3858 - val_accuracy: 0.8898\n",
            "Epoch 73/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1293 - accuracy: 0.9542 - val_loss: 0.3834 - val_accuracy: 0.8886\n",
            "Epoch 74/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1303 - accuracy: 0.9541 - val_loss: 0.3529 - val_accuracy: 0.8965\n",
            "Epoch 75/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1306 - accuracy: 0.9532 - val_loss: 0.3652 - val_accuracy: 0.8918\n",
            "Epoch 76/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1303 - accuracy: 0.9541 - val_loss: 0.3820 - val_accuracy: 0.8904\n",
            "Epoch 77/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1246 - accuracy: 0.9557 - val_loss: 0.3859 - val_accuracy: 0.8897\n",
            "Epoch 78/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1313 - accuracy: 0.9546 - val_loss: 0.3510 - val_accuracy: 0.8988\n",
            "Epoch 79/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1237 - accuracy: 0.9568 - val_loss: 0.3641 - val_accuracy: 0.8921\n",
            "Epoch 80/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1265 - accuracy: 0.9546 - val_loss: 0.3558 - val_accuracy: 0.8943\n",
            "Epoch 81/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1224 - accuracy: 0.9573 - val_loss: 0.3857 - val_accuracy: 0.8898\n",
            "Epoch 82/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1234 - accuracy: 0.9558 - val_loss: 0.3779 - val_accuracy: 0.8904\n",
            "Epoch 83/100\n",
            "300/300 [==============================] - 76s 253ms/step - loss: 0.1246 - accuracy: 0.9563 - val_loss: 0.3886 - val_accuracy: 0.8878\n",
            "Epoch 84/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1213 - accuracy: 0.9567 - val_loss: 0.3633 - val_accuracy: 0.8941\n",
            "Epoch 85/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1210 - accuracy: 0.9569 - val_loss: 0.3518 - val_accuracy: 0.8965\n",
            "Epoch 86/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1193 - accuracy: 0.9583 - val_loss: 0.3609 - val_accuracy: 0.8960\n",
            "Epoch 87/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1191 - accuracy: 0.9589 - val_loss: 0.3652 - val_accuracy: 0.8946\n",
            "Epoch 88/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1219 - accuracy: 0.9574 - val_loss: 0.3621 - val_accuracy: 0.8938\n",
            "Epoch 89/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1161 - accuracy: 0.9593 - val_loss: 0.3570 - val_accuracy: 0.8935\n",
            "Epoch 90/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1159 - accuracy: 0.9596 - val_loss: 0.3571 - val_accuracy: 0.8965\n",
            "Epoch 91/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1173 - accuracy: 0.9582 - val_loss: 0.3773 - val_accuracy: 0.8936\n",
            "Epoch 92/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1177 - accuracy: 0.9572 - val_loss: 0.3815 - val_accuracy: 0.8927\n",
            "Epoch 93/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1179 - accuracy: 0.9578 - val_loss: 0.3660 - val_accuracy: 0.8963\n",
            "Epoch 94/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1119 - accuracy: 0.9611 - val_loss: 0.3602 - val_accuracy: 0.8982\n",
            "Epoch 95/100\n",
            "300/300 [==============================] - 75s 251ms/step - loss: 0.1137 - accuracy: 0.9594 - val_loss: 0.3512 - val_accuracy: 0.8989\n",
            "Epoch 96/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1179 - accuracy: 0.9595 - val_loss: 0.3613 - val_accuracy: 0.8946\n",
            "Epoch 97/100\n",
            "300/300 [==============================] - 76s 252ms/step - loss: 0.1130 - accuracy: 0.9606 - val_loss: 0.3884 - val_accuracy: 0.8910\n",
            "Epoch 98/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1134 - accuracy: 0.9593 - val_loss: 0.3645 - val_accuracy: 0.8961\n",
            "Epoch 99/100\n",
            "300/300 [==============================] - 75s 252ms/step - loss: 0.1103 - accuracy: 0.9615 - val_loss: 0.3742 - val_accuracy: 0.8926\n",
            "Epoch 100/100\n",
            "300/300 [==============================] - 76s 253ms/step - loss: 0.1109 - accuracy: 0.9607 - val_loss: 0.4200 - val_accuracy: 0.8874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oK2DxQn2mTF",
        "colab_type": "text"
      },
      "source": [
        "#Increasing speed GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vniNRhp2l8c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "b370b84e-5fc0-4b27-bb57-e217b2790d28"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import timeit\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  print(\n",
        "      '\\n\\nThis error most likely means that this notebook is not '\n",
        "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
        "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
        "  raise SystemError('GPU device not found')\n",
        "\n",
        "def cpu():\n",
        "  with tf.device('/cpu:0'):\n",
        "    random_image_cpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)\n",
        "    return tf.math.reduce_sum(net_cpu)\n",
        "\n",
        "def gpu():\n",
        "  with tf.device('/device:GPU:0'):\n",
        "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
        "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
        "    return tf.math.reduce_sum(net_gpu)\n",
        "  \n",
        "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
        "cpu()\n",
        "gpu()\n",
        "\n",
        "# Run the op several times.\n",
        "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
        "      '(batch x height x width x channel). Sum of ten runs.')\n",
        "print('CPU (s):')\n",
        "cpu_time = timeit.timeit('cpu()', number=10, setup=\"from __main__ import cpu\")\n",
        "print(cpu_time)\n",
        "print('GPU (s):')\n",
        "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
        "print(gpu_time)\n",
        "print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
            "CPU (s):\n",
            "3.395748314000002\n",
            "GPU (s):\n",
            "0.04878333900001053\n",
            "GPU speedup over CPU: 69x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrO-pKozdKaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "669b9c74-221d-4527-d743-61e33c3e8dfe"
      },
      "source": [
        "# Test the model\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 8s 25ms/step - loss: 0.5384 - accuracy: 0.8439\n",
            "Test loss: 0.5384398698806763\n",
            "Test accuracy: 0.8439000248908997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yr4pE_LO1p_L",
        "colab_type": "text"
      },
      "source": [
        "## **Conclusion:**\n",
        "\n",
        "Test Accuracy: 84.39 \\\\\n",
        "Validation Accuracy: 88.74 \\\\\n",
        "Epochs: 200 \\\\\n",
        "Train Accuracy: 96.07 \\\\\n"
      ]
    }
  ]
}